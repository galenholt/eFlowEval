---
title: "Local_Inundation"
author: "Galen"
format: html
editor: visual
---

## Local runs of inundation

This could really be cleaned up even quite a bit more- it really just only takes a couple args and could be a function. And possibly wrapped in a while (!is.null(runlist)) (though that's dangerous)- see the post-processing at the end.

### Setup

```{r}
# Yeesh, pull these out
library(here)
library(tidyverse)
library(sf)
library(stars)
library(foreach)
library(doFuture)
```

First, set directories and read-in function

```{r}
source('directorySet.R')

registerDoFuture()

plan(multisession) # I *think* sequential allows debug?
```

### Check what needs to happen

```{r}
# I've modified makeSHfails to give me a list
runlist <- makeSHfails(outerDir = file.path(datOut, 'Inundationprocessed'),
            summaryFuns = 'lippiaAdultSurvive',
            varName = 'LippiaSurvive',
            nchunks = 100,
            lengthOrChunk = c('short', 'long'), # , 'long', 'chunk'
            runImmediate = FALSE,
            forceAllCatchments = TRUE,
            returnForR = TRUE)
```

```{r}
runlist
```

## Set up the runs

```{r}
#| messages: false
#| warning: false
# Run over the catchments and chunks 
alltimes <- foreach(w = names(runlist), 
                    .combine = rbind, 
                    .errorhandling = 'remove') %:%
  foreach(i = as.character(runlist[[w]]), 
          .combine = rbind, 
          .errorhandling = 'remove') %dopar% {
            inuntab <- processData(dataname = 'inundation', 
                                   data_dir = datDir, 
                                   summaryFun = 'lippiaAdultSurvive', 
                                   out_dir = datOut, 
                                   catchment = w, 
                                   thischunk = i)
            
            inuntab
          }
```

### Timings

The timings are saved in alltimes

```{r}
alltimes
```

# TEST-check BorderRivers for issues

On doing strictures, these kept not matching the ANAEs and having weird duplicates.

```{r}

bchunks <- list.files(file.path(datOut, 'Inundationprocessed', 'lippiaAdultSurvive', 'chunked', 'BorderRivers'))
```

```{r}
bdups <- foreach(i = bchunks) %do% {
  path <- file.path(datOut, 'Inundationprocessed', 
                 'lippiaAdultSurvive', 
                 'chunked', 'BorderRivers', i)
  chnames1 <- stringr::str_extract(i, '^.*(?=\\.rdata$)')
  chnames2 <- stringr::str_replace(chnames1, 'Survive_', 'Survive_index_')
  
  ch <- load_rename(path, 
                    knownnames = c(chnames1, chnames2),
                    newnames = c('aggdata', 'indices'))
  
  dups <- which(duplicated(ch$indices$UID))
}

which(!purrr::map_lgl(bdups, is_empty))
```

OK, no duplicates in any particular chunk

Let's concatenate, and see if that's where there's an issue. Steal the relevant bits from concatANAEchunks

Read in as list- this is almost the same as above, but modifing to work without refactoring that code

```{r}

partnames <- stringr::str_extract(bchunks, '^.*(?=\\.rdata$)')
indexnames <- stringr::str_replace(partnames, 'Survive_', 'Survive_index_')
  
  
dpList <- foreach(s = 1:length(bchunks)) %dopar% {
        load(file.path(datOut, 'Inundationprocessed', 
                 'lippiaAdultSurvive', 
                 'chunked', 'BorderRivers', bchunks[s]))
        # outl <- list()
        starpart <- get(partnames[s])
        indexpart <- get(indexnames[s])
        outl <- list(starpart, indexpart)
      }
```

Check null?

```{r}
nullindices <- foreach(l = 1:length(dpList), .combine = c) %do% {
        if (is.null(dpList[[l]][[1]]) & 
            (is.null(dpList[[l]][[2]]) || nrow(dpList[[l]][[2]]) == 0)) { # either null indices or a 0-lenght tibble
          l
        } else {
          NULL
        }
      }
```

Unpack and concatenate

```{r}
# Then, unpack the lists also using foreach
      depthAns <- foreach(l = 1:length(dpList),
                          .combine=function(...) c(..., along = 1), # Pass dimension argument to c.stars
                          .multicombine=TRUE,
                          .inorder = TRUE) %dopar% {
                            dpList[[l]][[1]]
                          }
      
      depthIndex <- foreach(l = 1:length(dpList),
                            .combine=bind_rows,
                            .multicombine=TRUE,
                            .inorder = TRUE) %dopar% {
                              dpList[[l]][[2]]
                            }
```

Check for duplicates

```{r}
which(duplicated(depthIndex$UID))
```

Well, that's odd. It also only has 15550 rows instead of 15552. Which is correct. So what happened? Was there a processing error somewhere?

## Concatenate

**TODO- make this automatic (a function?)**- I should be able to run \`makeSHfails\` here, then if there are no fails, immediately run `concatANAEchunks`. Could wrap the whole script in a `while(!is.null(runlist))`, but that's dangerous.

```{r}
runlist_end <- makeSHfails(outerDir = file.path(datOut, 'Inundationprocessed'),
                       summaryFuns = 'lippiaAdultSurvive',
                       varName = 'LippiaSurvive',
                       nchunks = 100,
                       lengthOrChunk = c('short', 'long'), # , 'long', 'chunk'
                       runImmediate = FALSE,
                       forceAllCatchments = TRUE,
                       returnForR = TRUE)

if (is.null(runlist_end)) {
  cattime <- system.time(concatANAEchunks(
    outerDir = file.path(datOut,
                         'Inundationprocessed'),
    summaryFuns = c('lippiaAdultSurvive')))
  print(cattime)
}


```

     user  system elapsed 
      91.36   61.53 1479.17 
